{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "(usage)=\n",
    "# Usage\n",
    "\n",
    "This page explains how to use VirtualiZarr today, by introducing the key concepts one-by-one.\n",
    "\n",
    "## Opening files as virtual datasets\n",
    "\n",
    "VirtualiZarr is for manipulating \"virtual\" references to pre-existing data stored on disk in a variety of formats, by representing it in terms of the [Zarr data model](https://zarr-specs.readthedocs.io/en/latest/specs.html) of chunked N-dimensional arrays.\n",
    "\n",
    "If we have a pre-existing netCDF file on disk:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Downloading xarray tutorial data requires pooch and netCDF4. This example also requries `h5py` and `h5netcdf` These can be installed with\n",
    "\n",
    "```bash\n",
    "pip install pooch netCDF4 h5py h5netcdf\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# rendering for the docs\n",
    "# xr.set_options(display_style=\"text\")\n",
    "\n",
    "# create an example pre-existing netCDF4 file\n",
    "ds = xr.tutorial.open_dataset('air_temperature')\n",
    "ds.to_netcdf('air.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We can open a virtual representation of this file using {py:func}`open_virtual_dataset <virtualizarr.open_virtual_dataset>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from virtualizarr import open_virtual_dataset\n",
    "\n",
    "vds = open_virtual_dataset('air.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "(Notice we did not have to explicitly indicate the file format, as {py:func}`open_virtual_dataset <virtualizarr.open_virtual_dataset>` will attempt to automatically infer it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "```{note}\n",
    "In future we would like for it to be possible to just use `xr.open_dataset`, e.g.\n",
    "\n",
    "    import virtualizarr\n",
    "\n",
    "    vds = xr.open_dataset('air.nc', engine='virtualizarr')\n",
    "\n",
    "but this requires some [upstream changes](https://github.com/TomNicholas/VirtualiZarr/issues/35) in xarray.\n",
    "```\n",
    "\n",
    "Printing this \"virtual dataset\" shows that although it is an instance of `xarray.Dataset`, unlike a typical xarray dataset, it does not contain numpy or dask arrays, but instead it wraps {py:class}`ManifestArray <virtualizarr.manifests.ManifestArray>` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "vds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Generally a \"virtual dataset\" is any `xarray.Dataset` which wraps one or more {py:class}`ManifestArray <virtualizarr.manifests.ManifestArray>` objects.\n",
    "\n",
    "These particular {py:class}`ManifestArray <virtualizarr.manifests.ManifestArray>` objects are each a virtual reference to some data in the `air.nc` netCDF file, with the references stored in the form of \"Chunk Manifests\".\n",
    "\n",
    "As the manifest contains only addresses at which to find large binary chunks, the virtual dataset takes up far less space in memory than the original dataset does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "ds.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "vds.virtualize.nbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "```{important} Virtual datasets are not normal xarray datasets!\n",
    "\n",
    "Although the top-level type is still `xarray.Dataset`, they are intended only as an abstract representation of a set of data files, not as something you can do analysis with. If you try to load, view, or plot any data you will get a `NotImplementedError`. Virtual datasets only support a very limited subset of normal xarray operations, particularly functions and methods for concatenating, merging and extracting variables, as well as operations for renaming dimensions and variables.\n",
    "\n",
    "_The only use case for a virtual dataset is [combining references](#combining-virtual-datasets) to files before [writing out those references to disk](#writing-virtual-stores-to-disk)._\n",
    "```\n",
    "\n",
    "### Opening remote files\n",
    "\n",
    "To open remote files as virtual datasets pass the `reader_options` options, e.g.\n",
    "\n",
    "```python\n",
    "aws_credentials = {\"key\": ..., \"secret\": ...}\n",
    "vds = open_virtual_dataset(\n",
    "    \"s3://some-bucket/file.nc\",\n",
    "    reader_options={'storage_options': aws_credentials}\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Chunk Manifests\n",
    "\n",
    "In the Zarr model N-dimensional arrays are stored as a series of compressed chunks, each labelled by a chunk key which indicates its position in the array. Whilst conventionally each of these Zarr chunks are a separate compressed binary file stored within a Zarr Store, there is no reason why these chunks could not actually already exist as part of another file (e.g. a netCDF file), and be loaded by reading a specific byte range from this pre-existing file.\n",
    "\n",
    "A \"Chunk Manifest\" is a list of chunk keys and their corresponding byte ranges in specific files, grouped together such that all the chunks form part of one Zarr-like array. For example, a chunk manifest for a 3-dimensional array made up of 4 chunks might look like this:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"0.0.0\": {\"path\": \"s3://bucket/foo.nc\", \"offset\": 100, \"length\": 100},\n",
    "    \"0.0.1\": {\"path\": \"s3://bucket/foo.nc\", \"offset\": 200, \"length\": 100},\n",
    "    \"0.1.0\": {\"path\": \"s3://bucket/foo.nc\", \"offset\": 300, \"length\": 100},\n",
    "    \"0.1.1\": {\"path\": \"s3://bucket/foo.nc\", \"offset\": 400, \"length\": 100},\n",
    "}\n",
    "```\n",
    "\n",
    "Notice that the `\"path\"` attribute points to a netCDF file `\"foo.nc\"` stored in a remote S3 bucket. There is no need for the files the chunk manifest refers to to be local.\n",
    "\n",
    "Our virtual dataset we opened above contains multiple chunk manifests stored in-memory, which we can see by pulling one out as a python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "marr = vds['air'].data\n",
    "manifest = marr.manifest\n",
    "manifest.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "In this case we can see that the `\"air\"` variable contains only one chunk, the bytes for which live in the `file:///work/data/air.nc` file, at the location given by the `'offset'` and `'length'` attributes.\n",
    "\n",
    "The {py:class}`ChunkManifest <virtualizarr.manifests.ChunkManifest>` class is virtualizarr's internal in-memory representation of this manifest.\n",
    "\n",
    "## `ManifestArray` class\n",
    "\n",
    "A Zarr array is defined not just by the location of its constituent chunk data, but by its array-level attributes such as `shape` and `dtype`. The {py:class}`ManifestArray <virtualizarr.manifests.ManifestArray>` class stores both the array-level attributes and the corresponding chunk manifest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "marr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "marr.manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "marr.zarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "A `ManifestArray` can therefore be thought of as a virtualized representation of a single Zarr array.\n",
    "\n",
    "As it defines various array-like methods, a `ManifestArray` can often be treated like a [\"duck array\"](https://docs.xarray.dev/en/stable/user-guide/duckarrays.html). In particular, concatenation of multiple `ManifestArray` objects can be done via merging their chunk manifests into one (and re-labelling the chunk keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "concatenated = np.concatenate([marr, marr], axis=0)\n",
    "concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "concatenated.manifest.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "This concatenation property is what will allow us to combine the data from multiple netCDF files on disk into a single Zarr store containing arrays of many chunks.\n",
    "\n",
    "```{note}\n",
    "As a single Zarr array has only one array-level set of compression codecs by definition, concatenation of arrays from files saved to disk with differing codecs cannot be achieved through concatenation of `ManifestArray` objects. Implementing this feature will require a more abstract and general notion of concatenation, see [GH issue #5](https://github.com/TomNicholas/VirtualiZarr/issues/5).\n",
    "```\n",
    "\n",
    "Remember that you cannot load values from a `ManifestArray` directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "\n",
    "```{code-cell}\n",
    ":tags: [raises-exception]\n",
    "\n",
    "print(thisvariabledoesntexist)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "vds['air'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The whole point is to manipulate references to the data without actually loading any data.\n",
    "\n",
    "```{note}\n",
    "You also cannot currently index into a `ManifestArray`, as arbitrary indexing would require loading data values to create the new array. We could imagine supporting indexing without loading data when slicing only along chunk boundaries, but this has not yet been implemented (see [GH issue #51](https://github.com/TomNicholas/VirtualiZarr/issues/51)).\n",
    "```\n",
    "\n",
    "## Virtual Datasets as Zarr Groups\n",
    "\n",
    "The full Zarr model (for a single group) includes multiple arrays, array names, named dimensions, and arbitrary dictionary-like attrs on each array. Whilst the duck-typed `ManifestArray` cannot store all of this information, an `xarray.Dataset` wrapping multiple `ManifestArray`s maps neatly to the Zarr model. This is what the virtual dataset we opened represents - all the information in one entire Zarr group, but held as references to on-disk chunks instead of as in-memory arrays.\n",
    "\n",
    "The problem of combining many archival format files (e.g. netCDF files) into one virtual Zarr store therefore becomes just a matter of opening each file using `open_virtual_dataset` and using [xarray's various combining functions](https://docs.xarray.dev/en/stable/user-guide/combining.html) to combine them into one aggregate virtual dataset.\n",
    "\n",
    "But before we combine our data, we might want to consider loading some variables into memory.\n",
    "\n",
    "## Loading variables\n",
    "\n",
    "Whilst the values of virtual variables (i.e. those backed by `ManifestArray` objects) cannot be loaded into memory, you do have the option of opening specific variables from the file as loadable lazy numpy/dask arrays, just like `xr.open_dataset` normally returns. These variables are specified using the `loadable_variables` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vds = open_virtual_dataset('air.nc', loadable_variables=['air', 'time'])\n",
    "vds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "You can see that the dataset contains a mixture of virtual variables backed by `ManifestArray` objects (`lat` and `lon`), and loadable variables backed by (lazy) numpy arrays (`air` and `time`).\n",
    "\n",
    "Loading variables can be useful in a few scenarios:\n",
    "1. You need to look at the actual values of a multi-dimensional variable in order to decide what to do next,\n",
    "2. You want in-memory indexes to use with ``xr.combine_by_coords``,\n",
    "3. Storing a variable on-disk as a set of references would be inefficient, e.g. because it's a very small array (saving the values like this is similar to kerchunk's concept of \"inlining\" data),\n",
    "4. The variable has encoding, and the simplest way to decode it correctly is to let xarray's standard decoding machinery load it into memory and apply the decoding,\n",
    "5. Some of your variables have inconsistent-length chunks, and you want to be able to concatenate them together. For example you might have multiple virtual datasets with coordinates of inconsistent length (e.g., leap years within multi-year daily data).\n",
    "\n",
    "### Loading low-dimensional coordinates\n",
    "\n",
    "In general, it is recommended to load all of your low-dimensional coordinates.\n",
    "This will slow down your initial opening of the individual virtual datasets, but by loading your coordinates into memory, they can be inlined in the reference file for fast reads of the virtualized store.\n",
    "However, doing this for coordinates that are N-dimensional might use a lot of storage duplicating them.\n",
    "Also, anything duplicated could become out of sync with the referenced original files, especially if not using a transactional storage engine like `Icechunk`.\n",
    "\n",
    "### CF-encoded time variables\n",
    "\n",
    "To correctly decode time variables according to the CF conventions, you need to pass `time` to `loadable_variables` and ensure the `decode_times` argument of `open_virtual_dataset` is set to True (`decode_times` defaults to None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vds = open_virtual_dataset(\n",
    "    'air.nc',\n",
    "    loadable_variables=['air', 'time'],\n",
    "    decode_times=True,\n",
    ")\n",
    "vds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Combining virtual datasets\n",
    "\n",
    "In general we should be able to combine all the datasets from our archival files into one using some combination of calls to `xarray.concat` and `xarray.merge`. For combining along multiple dimensions in one call we also have `xarray.combine_nested` and `xarray.combine_by_coords`. If you're not familiar with any of these functions we recommend you skim through [xarray's docs on combining](https://docs.xarray.dev/en/stable/user-guide/combining.html).\n",
    "\n",
    "Let's create two new netCDF files, which we would need to open and concatenate in a specific order to represent our entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds1 = ds.isel(time=slice(None, 1460))\n",
    "ds2 = ds.isel(time=slice(1460, None))\n",
    "\n",
    "ds1.to_netcdf('air1.nc')\n",
    "ds2.to_netcdf('air2.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Note that we have created these in such a way that each dataset has one equally-sized chunk.\n",
    "\n",
    "TODO: Note about variable-length chunking?\n",
    "\n",
    "### Manual concatenation ordering\n",
    "\n",
    "The simplest case of concatenation is when you have a set of files and you know which order they should be concatenated in, _without looking inside the files_. In this case it is sufficient to open the files one-by-one, then pass the virtual datasets as a list to the concatenation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vds1 = open_virtual_dataset('air1.nc')\n",
    "vds2 = open_virtual_dataset('air2.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "As we know the correct order a priori, we can just combine along one dimension using `xarray.concat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_vds = xr.concat([vds1, vds2], dim='time', coords='minimal', compat='override')\n",
    "combined_vds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_vds['air'].data.manifest.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "```{note}\n",
    "The keyword arguments `coords='minimal', compat='override'` are currently necessary because the default behaviour of xarray will attempt to load coordinates in order to check their compatibility with one another. In future this [default will be changed](https://github.com/pydata/xarray/issues/8778), such that passing these two arguments explicitly will become unnecessary.\n",
    "```\n",
    "\n",
    "The general multi-dimensional version of this concatenation-by-order-supplied can be achieved using `xarray.combine_nested`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_vds = xr.combine_nested([vds1, vds2], concat_dim=['time'], coords='minimal', compat='override')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In N-dimensions the datasets would need to be passed as an N-deep nested list-of-lists, see the [xarray docs](https://docs.xarray.dev/en/stable/user-guide/combining.html#combining-along-multiple-dimensions).\n",
    "\n",
    "```{note}\n",
    "In future we would like for it to be possible to just use `xr.open_mfdataset` to open the files and combine them in one go, e.g.\n",
    "\n",
    "    vds = xr.open_mfdataset(\n",
    "        ['air1.nc', 'air2.nc'],\n",
    "        combine='nested',\n",
    "        concat_dim=['time'],\n",
    "        coords='minimal',\n",
    "        compat='override',\n",
    "    )\n",
    "\n",
    "but this requires some [upstream changes](https://github.com/TomNicholas/VirtualiZarr/issues/35) in xarray.\n",
    "```\n",
    "\n",
    "```{note}\n",
    "For manual concatenation we can actually avoid creating any xarray indexes, as we won't need them. Without indexes we can avoid loading any data whatsoever from the files. However, you should first be confident that the archival files actually do have compatible data, as the coordinate values then cannot be efficiently compared for consistency (i.e. aligned).\n",
    "\n",
    "By default indexes are created for 1-dimensional ``loadable_variables`` whose name matches their only dimension (i.e. \"dimension coordinates\"), but if you wish you can load variables without creating any indexes by passing ``indexes={}`` to ``open_virtual_dataset``.\n",
    "```\n",
    "\n",
    "### Ordering by coordinate values\n",
    "\n",
    "If you're happy to load 1D dimension coordinates into memory, you can use their values to do the ordering for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vds1 = open_virtual_dataset('air1.nc', loadable_variables=['time', 'lat', 'lon'])\n",
    "vds2 = open_virtual_dataset('air2.nc', loadable_variables=['time', 'lat', 'lon'])\n",
    "\n",
    "combined_vds = xr.combine_by_coords([vds2, vds1], coords='minimal', compat='override')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Notice we don't have to specify the concatenation dimension explicitly - xarray works out the correct ordering for us. Even though we actually passed in the virtual datasets in the wrong order just now, the manifest still has the chunks listed in the correct order such that the 1-dimensional ``time`` coordinate has ascending values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_vds['air'].data.manifest.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Ordering using metadata\n",
    "\n",
    "TODO: Use preprocess to create a new index from the metadata. Requires ``open_virtual_mfdataset`` to be implemented in [PR #349](https://github.com/zarr-developers/VirtualiZarr/pull/349).\n",
    "\n",
    "## Writing virtual stores to disk\n",
    "\n",
    "Once we've combined references to all the chunks of all our archival files into one virtual xarray dataset, we still need to write these references out to disk so that they can be read by our analysis code later.\n",
    "\n",
    "### Writing to Kerchunk's format and reading data via fsspec\n",
    "\n",
    "The [kerchunk library](https://github.com/fsspec/kerchunk) has its own [specification](https://fsspec.github.io/kerchunk/spec.html) for how byte range references should be serialized (either as a JSON or parquet file).\n",
    "\n",
    "To write out all the references in the virtual dataset as a single kerchunk-compliant JSON or parquet file, you can use the {py:meth}`vds.virtualize.to_kerchunk <virtualizarr.VirtualiZarrDatasetAccessor.to_kerchunk>` accessor method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_vds.virtualize.to_kerchunk('combined.json', format='json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "These references can now be interpreted like they were a Zarr store by [fsspec](https://github.com/fsspec/filesystem_spec), using kerchunk's built-in xarray backend (kerchunk must be installed to use `engine='kerchunk'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_ds = xr.open_dataset('combined.json', engine=\"kerchunk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In-memory (\"loadable\") variables backed by numpy arrays can also be written out to kerchunk reference files, with the values serialized as bytes. This is equivalent to kerchunk's concept of \"inlining\", but done on a per-array basis using the `loadable_variables` kwarg rather than a per-chunk basis using kerchunk's `inline_threshold` kwarg.\n",
    "\n",
    "```{note}\n",
    "Currently you can only serialize in-memory variables to kerchunk references if they do not have any encoding.\n",
    "```\n",
    "\n",
    "When you have many chunks, the reference file can get large enough to be unwieldy as json. In that case the references can be instead stored as parquet. Again this uses kerchunk internally.\n",
    "\n",
    "\n",
    "```{note}\n",
    "to run this example you need `fastparquet`\n",
    "\n",
    "```bash\n",
    "pip install fastparquet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_vds.virtualize.to_kerchunk('combined.parquet', format='parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "And again we can read these references using the \"kerchunk\" backend as if it were a regular Zarr store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_ds = xr.open_dataset('combined.parquet', engine=\"kerchunk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "By default references are placed in separate parquet file when the total number of references exceeds `record_size`. If there are fewer than `categorical_threshold` unique urls referenced by a particular variable, url will be stored as a categorical variable.\n",
    "\n",
    "### Writing to an Icechunk Store\n",
    "\n",
    "We can also write these references out as an [IcechunkStore](https://icechunk.io/). `Icechunk` is an open-source, cloud-native transactional tensor storage engine that is compatible with Zarr version 3. To export our virtual dataset to an `Icechunk` Store, we simply use the {py:meth}`vds.virtualize.to_icechunk <virtualizarr.VirtualiZarrDatasetAccessor.to_icechunk>` accessor method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create an icechunk repository, session and write the virtual dataset to the session\n",
    "import icechunk\n",
    "import tempfile\n",
    "storage = icechunk.local_filesystem_storage(tempfile.TemporaryDirectory().name)\n",
    "\n",
    "# By default, local virtual references and public remote virtual references\n",
    "# can be read witout extra configuration.\n",
    "repo = icechunk.Repository.create(storage)\n",
    "session = repo.writable_session(\"main\")\n",
    "\n",
    "# write the virtual dataset to the session with the IcechunkStore\n",
    "vds1.virtualize.to_icechunk(session.store)\n",
    "session.commit(\"Wrote first dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "#### Append to an existing Icechunk Store\n",
    "\n",
    "You can append a virtual dataset to an existing Icechunk store using the `append_dim` argument. This is especially useful for datasets that grow over time. Note that Zarr does not currently support concatenating datasets with different codecs or chunk shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = repo.writable_session(\"main\")\n",
    "\n",
    "# write the virtual dataset to the session with the IcechunkStore\n",
    "vds2.virtualize.to_icechunk(session.store, append_dim=\"time\")\n",
    "session.commit(\"Appended second dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "See the [Icechunk documentation](https://icechunk.io/icechunk-python/virtual/#creating-a-virtual-dataset-with-virtualizarr) for more details.\n",
    "icechunk-python/virtual/#creating-a-virtual-dataset-with-virtualizarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "## Opening Kerchunk references as virtual datasets\n",
    "\n",
    "You can open existing Kerchunk `json` or `parquet` references as Virtualizarr virtual datasets. This may be useful for converting existing Kerchunk formatted references to storage formats like [Icechunk](https://icechunk.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "vds = open_virtual_dataset('combined.json', filetype='kerchunk')\n",
    "# or\n",
    "vds = open_virtual_dataset('combined.parquet', filetype='kerchunk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "One difference between the kerchunk references format and virtualizarr's internal manifest representation (as well as icechunk's format) is that paths in kerchunk references can be relative paths. Opening kerchunk references that contain relative local filepaths therefore requires supplying another piece of information: the directory of the ``fsspec`` filesystem which the filepath was defined relative to.\n",
    "\n",
    "You can dis-ambuiguate kerchunk references containing relative paths by passing the ``fs_root`` kwarg to ``virtual_backend_kwargs``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file `relative_refs.json` contains a path like './file.nc'\n",
    "\n",
    "vds = open_virtual_dataset(\n",
    "    'relative_refs.json',\n",
    "    filetype='kerchunk',\n",
    "    virtual_backend_kwargs={'fs_root': 'file:///some_directory/'}\n",
    ")\n",
    "\n",
    "# the path in the virtual dataset will now be 'file:///some_directory/file.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "Note that as the virtualizarr {py:meth}`vds.virtualize.to_kerchunk <virtualizarr.VirtualiZarrDatasetAccessor.to_kerchunk>` method only writes absolute paths, the only scenario in which you might come across references containing relative paths is if you are opening references that were previously created using the ``kerchunk`` library alone.\n",
    "\n",
    "## Rewriting existing manifests\n",
    "\n",
    "Sometimes it can be useful to rewrite the contents of an already-generated manifest or virtual dataset.\n",
    "\n",
    "### Rewriting file paths\n",
    "\n",
    "You can rewrite the file paths stored in a manifest or virtual dataset without changing the byte range information using the {py:meth}`vds.virtualize.rename_paths <virtualizarr.VirtualiZarrDatasetAccessor.rename_paths>` accessor method.\n",
    "\n",
    "For example, you may want to rename file paths according to a function to reflect having moved the location of the referenced files from local storage to an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "vds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def local_to_s3_url(old_local_path: str) -> str:\n",
    "    from pathlib import Path\n",
    "\n",
    "    new_s3_bucket_url = \"s3://my_bucket/\"\n",
    "\n",
    "    filename = Path(old_local_path).name\n",
    "    return new_s3_bucket_url + \"/\" + str(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "renamed_vds = vds.virtualize.rename_paths(local_to_s3_url)\n",
    "renamed_vds['air'].data.manifest.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_vds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "```\n",
    "{'0.0.0': {'path': 'http://s3.amazonaws.com/my_bucket/air.nc', 'offset': 15419, 'length': 7738000}}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
